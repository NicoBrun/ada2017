{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipeline for 'meta_amazon_instant_video.json' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='royalblue' size=3>\n",
    "<b>\n",
    "\n",
    "Parameters for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "datapath = '../DATA/'\n",
    "df_name = \"meta_Books\"\n",
    "\n",
    "\n",
    "###\n",
    "###  Change here which fields will be extracted from the amazon product\n",
    "###\n",
    "def set_df_cell_from_product(amazonProduct, rowNumber, fieldName1, fieldName2) :\n",
    "    if (amazonProduct is None) : \n",
    "        amazon_products_df.set_value(rowNumber, fieldName1, [])\n",
    "        amazon_products_df.set_value(rowNumber, fieldName2, [])     \n",
    "    else : \n",
    "        amazon_products_df.set_value(rowNumber, \"actors\",\n",
    "                                     amazonProduct.actors)\n",
    "        amazon_products_df.set_value(rowNumber, \"directors\",\n",
    "                                     amazonProduct.directors)\n",
    "        amazon_products_df.set_value(rowNumber, \"creators\", \n",
    "                                     amazonProduct.creators)\n",
    "        amazon_products_df.set_value(rowNumber, \"authors\", \n",
    "                                     amazonProduct.authors)\n",
    "        amazon_products_df.set_value(rowNumber, \"model\", \n",
    "                                     amazonProduct.model)\n",
    "        #amazon_products_df.set_value(rowNumber, \"artists\",    # There is definitely no \"artists\" field\n",
    "        #                             amazonProduct.artists)\n",
    "\n",
    "# How frequently should we save our data to file\n",
    "data_save_freq = 10000\n",
    "# How many network errors we accept before we give up\n",
    "network_errors_limit = 7\n",
    "# feature we're interested in\n",
    "features = ['asin', 'categories', 'description', 'title', 'salesRank']\n",
    "columns_to_add = ['actors', 'directors', 'creators', 'authors', \"model\"]\n",
    "json_name = df_name+'.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Sleep\n",
    "import time\n",
    "\n",
    "# Strict JSON conversion\n",
    "import json \n",
    "import gzip \n",
    "\n",
    "# Progress display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Amazon API querying\n",
    "from amazon.api import AmazonAPI\n",
    "from amazon.api import AsinNotFound\n",
    "\n",
    "from urllib.request import HTTPError\n",
    "from socket import gaierror\n",
    "from urllib.request import URLError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='royalblue' size=3>\n",
    "<b>\n",
    "\n",
    "Open metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load all ASINs we're going to query - use the metadata files\n",
    "## for this, as they contains each ASIN once and only once.\n",
    "##\n",
    "\n",
    "''' This function was provided on the amazon dataset's webpage\n",
    "    It loads a gzipped file directly into a dataframe\n",
    "'''\n",
    "def gz_to_dataframe(datapath, filename):\n",
    "    def parse(path): \n",
    "        g = gzip.open(path, 'rb') \n",
    "        for l in g: \n",
    "            yield eval(l) \n",
    "    def getDF(path): \n",
    "        i = 0 \n",
    "        df = {} \n",
    "        for d in parse(path): \n",
    "            df[i] = d \n",
    "            i += 1 \n",
    "        return pd.DataFrame.from_dict(df, orient='index') \n",
    "    return getDF(datapath+filename)\n",
    "    \n",
    "amazon_products_df = gz_to_dataframe(datapath, json_name)\n",
    "amazon_products_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon_products_df = amazon_products_df[features]\n",
    "amazon_products_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='royalblue' size=3>\n",
    "<b>\n",
    "\n",
    "Prepare for amazon api usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sign in with amazon API \n",
    "##\n",
    "\n",
    "def get_amazon_interface():\n",
    "    f = open(\"api_creds\")\n",
    "    ar = f.read().split(\"\\n\")\n",
    "    return AmazonAPI(ar[0], ar[1], ar[2])\n",
    "    return ar[0], ar[1], ar[2]\n",
    "\n",
    "amazon = get_amazon_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Here, we define some API query helpers\n",
    "##\n",
    "\n",
    "''' Product lookup with API, asin can be a string ('one by one' lookup)\n",
    "    or a list of strings ('bulk lookup').\n",
    "    bulk lookup provides better performance\n",
    "'''\n",
    "def get_prod(asin) : \n",
    "    if not isinstance(asin, str): \n",
    "        acc_str = asin[0]\n",
    "        for e in asin : \n",
    "            acc_str += ','+e\n",
    "        print(acc_str)\n",
    "        return amazon.lookup(ItemId=acc_str)\n",
    "    else :\n",
    "        return amazon.lookup(ItemId=asin)\n",
    "    \n",
    "''' Splits the interval [start-end] into bulks of size bulksize\n",
    "'''    \n",
    "def gen_bulk_index(start, end, bulksize=10, includeEnd=False):\n",
    "    size = end - start + 1\n",
    "    bulks = [list(range(start+(i*bulksize), start + (i+1)*bulksize)) for i in range(0, int(size/bulksize))]\n",
    "    if includeEnd : \n",
    "        bulks.append(list(range(bulks[len(bulks)-1][bulksize-1], end+1)))\n",
    "    else : \n",
    "        bulks.append(list(range(bulks[len(bulks)-1][bulksize-1], end)))\n",
    "    return bulks    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Also, we have some functions to save the state of our data structure - in case we need to \n",
    "## to shutdown the computer and restart the query loop at a later time (not used here)\n",
    "##\n",
    "\n",
    "def save_progress(dataframe, nb_rows_processed):\n",
    "    dataframe.to_csv(datapath+df_name+\"_temp.csv\", index=False)\n",
    "    file = open(datapath+df_name+\"_progress\", \"w\")\n",
    "    file.write(str(nb_rows_processed))\n",
    "\n",
    "    \n",
    "def load_progress():\n",
    "    dataframe = pd.read_csv(datapath+df_name+\"_temp.csv\")\n",
    "    file = open(datapath+df_name+\"_progress\", \"r\")\n",
    "    nb_rows_processed = file.readline()\n",
    "    return dataframe, int(nb_rows_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='royalblue' size=3>\n",
    "<b>\n",
    "\n",
    "Bulk lookups parameters and loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Parameters & initialization for bulk item lookup\n",
    "##\n",
    "\n",
    "bulksize = 10\n",
    "\n",
    "# Change this to restore progress from file\n",
    "fresh_run = True\n",
    "\n",
    "if fresh_run : \n",
    "    # used to restart from where we were in case of unexepected network error\n",
    "    lastItemLookedUp = 0\n",
    "    for col in columns_to_add : \n",
    "        amazon_products_df[col] = pd.Series(dtype=object)\n",
    "else : \n",
    "    amazon_products_df, lastItemLookedUp = load_progress()\n",
    "    \n",
    "print(\"last item looked up : \", lastItemLookedUp,  \"  -  time : \",time.strftime(\"%H:%M:%S\"), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Querying loop\n",
    "##\n",
    "\n",
    "ref_for_progress = lastItemLookedUp\n",
    "lastErrorMet=lastItemLookedUp\n",
    "loop_complete = False\n",
    "caught_httperrors=0\n",
    "caught_gaierrors=0\n",
    "errors_counter=0\n",
    "\n",
    "\n",
    "while not loop_complete : \n",
    "    try : \n",
    "        \n",
    "        for bulk in gen_bulk_index(ref_for_progress, amazon_products_df.shape[0], bulksize=bulksize) : \n",
    "            # update progess every 100 items\n",
    "            if ((bulk[0]-(ref_for_progress)) % 100 == 0) : \n",
    "                clear_output()\n",
    "                print(\"    \",int(100 * (bulk[0]+1) / amazon_products_df.shape[0]), \"% completed (\",bulk[0], \" rows)\", \"  -  time : \",time.strftime(\"%H:%M:%S\"))\n",
    "                print(\"         Last Item Looked up : \", lastItemLookedUp, \" / \", amazon_products_df.shape[0])\n",
    "                print(\"         HTTPErrors caught : \", caught_httperrors)\n",
    "                print(\"         gaierrors caught : \", caught_gaierrors)\n",
    "                print(\"\\n\\n\\n\")\n",
    "\n",
    "            # get asins for the bulk and fetch the matching AmazonProducts\n",
    "            asins = amazon_products_df['asin'][bulk].tolist()\n",
    "            noAsinFound = False\n",
    "            try : \n",
    "                prods = get_prod(asins)\n",
    "            except AsinNotFound : \n",
    "                noAsinFound = True\n",
    "                \n",
    "            # if query was successful, reset error counter\n",
    "            errors_counter = 0;\n",
    "            \n",
    "            # Then, process each product to add necessary informations in the dataframe\n",
    "            if noAsinFound : \n",
    "                # Skip this bulk and reset the flag\n",
    "                print(\"No Asin Found for bulk : \", bulk)\n",
    "            elif (type(prods) is list) and (len(prods) == bulksize) :              \n",
    "                # Case : we found exactly one result per ASIN\n",
    "                #        process by bulk\n",
    "                    for i, prod in enumerate(prods) : \n",
    "                        set_df_cell_from_product(prod, bulk[i], \"actors\", \"directors\")\n",
    "            elif (type(prods) is list) or (type(prods) is AmazonApi.AmazonProduct) :  \n",
    "                # Case : we obtained a list of AmazonProducts or a single AmazonProduct\n",
    "                #        fallback to 1-by-1 querying\n",
    "                for n in bulk :               \n",
    "                    asin = amazon_products_df['asin'][n]\n",
    "                    try : \n",
    "                        prod = get_prod(asin)\n",
    "                    except(AsinNotFound): \n",
    "                        prod = None\n",
    "                    set_df_cell_from_product(prod, n, \"actors\", \"directors\")\n",
    "                    time.sleep(0.5)\n",
    "\n",
    "            # Save progress\n",
    "            lastItemLookedUp = bulk[bulksize-1]\n",
    "            \n",
    "            # Save data to file according to specified frequency\n",
    "            if ((bulk[0]-(ref_for_progress)) % data_save_freq == 0) : \n",
    "                save_progress(amazon_products_df, lastItemLookedUp)\n",
    "            \n",
    "            # limit query frequency to avoid 503 errors\n",
    "            time.sleep(min(bulksize/25, 5))\n",
    "            \n",
    "        loop_complete = True\n",
    "        \n",
    "\n",
    "    except HTTPError :\n",
    "        errors_counter += 1\n",
    "        caught_httperrors += 1\n",
    "        # If we didn't make any progress, something must be wrong\n",
    "        if errors_counter > network_errors_limit : \n",
    "            print(\"HTTPError caught at original_lastItemLookedUp  -  breaking\")\n",
    "            break \n",
    "        # else retry\n",
    "        print(\"\\n\\nhttpError\\n\\n\")\n",
    "        ref_for_progress = lastItemLookedUp\n",
    "        lastErrorMet = lastItemLookedUp\n",
    "        \n",
    "    except(gaierror, URLError): \n",
    "        errors_counter += 1\n",
    "        caught_gaierrors += 1\n",
    "        if errors_counter > network_errors_limit : \n",
    "            print(\"gaierror/urlerror caught too many times  -  breaking\")\n",
    "            break \n",
    "        # else retry\n",
    "        print(\"\\n\\ngaierror/urlerror\\n\\n\")\n",
    "        ref_for_progress = lastItemLookedUp\n",
    "        lastErrorMet = lastItemLookedUp\n",
    "        \n",
    "\n",
    "if(loop_complete):\n",
    "    clear_output()    \n",
    "    # save results\n",
    "    amazon_products_df.to_csv(datapath+df_name+\".csv\")\n",
    "    print(\"amazon query loop completed !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amazon_products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run this cell to manually save results\n",
    "##\n",
    "\n",
    "save_progress(amazon_products_df, lastItemLookedUp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
