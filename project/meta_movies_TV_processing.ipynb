{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Processing pipeline for 'meta_Movies_and_TV.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = 'DATA/'\n",
    "filename = 'meta_Movies_and_TV.json.gz'\n",
    "df_name = 'meta_Movies_and_TV'\n",
    "\n",
    "features = ['asin', 'title', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Strict JSON conversion\n",
    "import json \n",
    "import gzip \n",
    "\n",
    "# Sleep\n",
    "import time\n",
    "\n",
    "# Progress display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from urllib.request import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gz_to_dataframe(datapath, filename):\n",
    "    def parse(path): \n",
    "        g = gzip.open(path, 'rb') \n",
    "        for l in g: \n",
    "            yield eval(l) \n",
    "    def getDF(path): \n",
    "        i = 0 \n",
    "        df = {} \n",
    "        for d in parse(path): \n",
    "            df[i] = d \n",
    "            i += 1 \n",
    "        return pd.DataFrame.from_dict(df, orient='index') \n",
    "    return getDF(datapath+filename)\n",
    "    \n",
    "product_df = gz_to_dataframe(datapath, filename)[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208321, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. fetch artists names with amazon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Identify with amazon api servers\n",
    "##\n",
    "\n",
    "from amazon.api import AmazonAPI\n",
    "from amazon.api import AsinNotFound\n",
    "\n",
    "def get_amazon_interface():\n",
    "    f = open(\"api_creds\")\n",
    "    ar = f.read().split(\"\\n\")\n",
    "    return AmazonAPI(ar[0], ar[1], ar[2])\n",
    "    return ar[0], ar[1], ar[2]\n",
    "\n",
    "amazon = get_amazon_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## API query helpers\n",
    "##\n",
    "\n",
    "\n",
    "''' Product lookup with API, asin can be a string ('one by one' lookup)\n",
    "    or a list of strings ('bulk lookup').\n",
    "    bulk lookup provides better performance\n",
    "'''\n",
    "def get_prod(asin) : \n",
    "    if not isinstance(asin, str): \n",
    "        acc_str = str(asin[0])\n",
    "        for i in range(1, len(asin)) : \n",
    "            acc_str += ','+str(asin[i])\n",
    "        print(acc_str)\n",
    "        return amazon.lookup(ItemId=acc_str)\n",
    "    else :\n",
    "        return amazon.lookup(ItemId=asin)\n",
    "    \n",
    "    \n",
    "''' Splits the interval [start-end] into bulks of size bulksize\n",
    "'''    \n",
    "def gen_bulk_index(start, end, bulksize=10, includeEnd=False):\n",
    "    size = end - start + 1\n",
    "    bulks = [list(range(start+(i*bulksize), start + (i+1)*bulksize)) for i in range(0, int(size/bulksize))]\n",
    "    if includeEnd : \n",
    "        bulks.append(list(range(bulks[len(bulks)-1][bulksize-1], end+1)))\n",
    "    else : \n",
    "        bulks.append(list(range(bulks[len(bulks)-1][bulksize-1], end)))\n",
    "    return bulks    \n",
    "\n",
    "\n",
    "''' Helper : extracts wanted data from the amazon product 'amazonProduct' and \n",
    "             puts it in the dataframe at the given positions   \n",
    "'''\n",
    "def set_df_cell_from_product(amazonProduct, rowNumber, fieldName1, fieldName2) :\n",
    "    if (amazonProduct is None) : \n",
    "        product_df.set_value(rowNumber, fieldName1, [])\n",
    "        product_df.set_value(rowNumber, fieldName2, [])     \n",
    "    else : \n",
    "        actors = get_actors(amazonProduct)\n",
    "        product_df.set_value(rowNumber, fieldName1, actors)\n",
    "        directors = get_directors(amazonProduct)\n",
    "        product_df.set_value(rowNumber, fieldName2, directors)\n",
    "\n",
    "\n",
    "''' Save and load progress to file, as opposed to keeping it in memory inbetween cells\n",
    "'''\n",
    "def save_progress(dataframe, nb_rows_processed):\n",
    "    dataframe.to_csv(datapath+\"meta_movies_TV_temp.csv\")\n",
    "    file = open(datapath+\"meta_movies_TV_progress\", \"w\")\n",
    "    file.write(str(nb_rows_processed))\n",
    "\n",
    "    \n",
    "def load_progress():\n",
    "    dataframe = pd.read_csv(datapath+\"meta_movies_TV_temp.csv\")\n",
    "    file = open(datapath+\"meta_movies_TV_progress\", \"r\")\n",
    "    nb_rows_processed = file.readline()\n",
    "    return dataframe, int(nb_rows_processed)\n",
    "\n",
    "\n",
    "def get_directors(prod) : \n",
    "    return prod.directors\n",
    "\n",
    "def get_actors(prod) : \n",
    "    return prod.actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API \"bulk\" querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last item looked up :  199134   -  time :  22:37:57 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2850: DtypeWarning: Columns (7,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "## Parameters for bulk item lookup, should be kept between runs\n",
    "##\n",
    "\n",
    "bulksize = 10\n",
    "\n",
    "fresh_run = False\n",
    "\n",
    "# used to restart from where we were in case of an error\n",
    "lastItemLookedUp = 0\n",
    "incompleterows = 0\n",
    "emptyrows = 0\n",
    "caught_httperrors = 0\n",
    "\n",
    "\n",
    "if fresh_run : \n",
    "    product_df = product_df[features]\n",
    "    product_df['actors'] = pd.Series(dtype=object)\n",
    "    product_df['directors'] = pd.Series(dtype=object)\n",
    "else : \n",
    "    product_df, lastItemLookedUp = load_progress()\n",
    "    \n",
    "print(\"last item looked up : \", lastItemLookedUp,  \"  -  time : \",time.strftime(\"%H:%M:%S\"), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208321, 15)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-37c590723808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbulk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_bulk_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastItemLookedUp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbulksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbulksize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m# display progess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbulk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_for_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-daaef2d2c5a3>\u001b[0m in \u001b[0;36mgen_bulk_index\u001b[0;34m(start, end, bulksize, includeEnd)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbulks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbulks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbulks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbulksize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mbulks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbulks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbulks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbulksize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbulks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lastItemLookedUp=\n",
    "\n",
    "ref_for_progress = lastItemLookedUp\n",
    "loop_complete = False\n",
    "\n",
    "\n",
    "while not loop_complete : \n",
    "    try : \n",
    "        \n",
    "        for bulk in gen_bulk_index(lastItemLookedUp, product_df.shape[0], bulksize=bulksize) : \n",
    "            # display progess\n",
    "            if ((bulk[0]-(ref_for_progress)) % 100 == 0) : \n",
    "                clear_output()\n",
    "                print(\"    \",int(100 * (bulk[0]+1) / product_df.shape[0]), \"% completed (\",bulk[0], \" rows)\", \"  -  time : \",time.strftime(\"%H:%M:%S\"))\n",
    "                print(\"         Last Item Looked up : \", lastItemLookedUp, \" / \", product_df.shape[0])\n",
    "                print(\"         Incomplete rows : \", incompleterows)\n",
    "                print(\"         Empty rows : \", emptyrows)\n",
    "                print(\"         HTTPErrors caught : \", caught_httperrors)\n",
    "                print(\"\\n\\n\\n\")\n",
    "\n",
    "            # get asins for the bulk and fetch the matching AmazonProducts\n",
    "            noAsinFound = False\n",
    "            asins = product_df['asin'][bulk].tolist()\n",
    "            try : \n",
    "                prods = get_prod(asins)\n",
    "            except AsinNotFound : \n",
    "                noAsinFound = True\n",
    "\n",
    "            \n",
    "            if noAsinFound : \n",
    "                # Skip this bulk and reset the flag\n",
    "                print(\"No Asin Found for bulk : \", bulk)\n",
    "            elif (type(prods) is list) and (len(prods) == bulksize) :              \n",
    "                # Case : we found exactly one result per ASIN\n",
    "                #        process by bulk\n",
    "                    for i, prod in enumerate(prods) : \n",
    "                        set_df_cell_from_product(prod, bulk[i], \"actors\", \"directors\")\n",
    "            elif (type(prods) is list) or (type(prods) is amazon.api.AmazonProduct) :  \n",
    "                # Case : we obtained a list of AmazonProducts or a single AmazonProduct\n",
    "                #        fallback to 1-by-1 querying\n",
    "                for n in bulk :               \n",
    "                    asin = product_df['asin'][n]\n",
    "                    try : \n",
    "                        prod = get_prod(asin)\n",
    "                    except(AsinNotFound): \n",
    "                        prod = None\n",
    "                    set_df_cell_from_product(prod, n, \"actors\", \"directors\")\n",
    "                    time.sleep(0.5)\n",
    "                incompleterows += 1\n",
    "\n",
    "            # Save progress\n",
    "            lastItemLookedUp = bulk[len(bulk)-1]\n",
    "\n",
    "            # limit query frequency to avoid 503 errors\n",
    "            time.sleep(min(bulksize/40, 10))\n",
    "            \n",
    "        loop_complete = True\n",
    "            \n",
    "    except HTTPError :\n",
    "        # Nothing to do, loop_complete is still false,\n",
    "        # We will go back to the query loop and start from the last recorded lastItemLookedUp\n",
    "        \n",
    "        # If we didn't make any progress, something must be wrong\n",
    "        if lastItemLookedUp == ref_for_progress : \n",
    "            print(\"HTTPError caught at original_lastItemLookedUp  -  breaking\")\n",
    "            break \n",
    "        # else retry\n",
    "        print()\n",
    "        print(httperror)\n",
    "        print()\n",
    "        caught_httperrors += 1\n",
    "        ref_for_progress = lastItemLookedUp\n",
    "    \n",
    "clear_output()\n",
    "print(\"API query loop completed ! \")\n",
    "product_df.to_csv(datapath+df_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_progress(product_df, lastItemLookedUp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
