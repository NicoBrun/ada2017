{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [1. Introduction](#introduction)\n",
    "* [2. Data gathering](#data)\n",
    "* * [2.1 Amazon dataset](#amazon)\n",
    "* * [2.2 scraping Wikipedia](#wikipedia)\n",
    "* * [2.3 Amazon Product API](#amazonapi)\n",
    "* [3. What's next for Milestone #3](#milestone3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#scraping imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#plotting imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#String matching\n",
    "import re\n",
    "\n",
    "# Date\n",
    "import datetime as dt\n",
    "# Sleep\n",
    "import time\n",
    "\n",
    "# Strict JSON conversion\n",
    "import json \n",
    "import gzip \n",
    "\n",
    "# Progress display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Amazon API querying\n",
    "from amazon.api import AmazonAPI\n",
    "from amazon.api import AsinNotFound\n",
    "\n",
    "datapath = 'DATA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "# 1. Introduction\n",
    "\n",
    "## Abstract\n",
    "\n",
    "It is often said, ironically, that Van Gogh never sold a painting in his lifetime while he is one of the most famous painters in history.\n",
    "Is this an isolated case? Is it possible that society has a greater interest in the works of deceased personalities rather than those of their contemporaries? And if so, is this interest more marked when the news is still fresh?\n",
    "The following project aims to analyze the effect of artists / authors’ death on sales of their own work. \n",
    "It starts with the hypothesis that a real societal phenomenon exists, which we will call \"post-mortem worship\", according to which people feel more interested in the works of artists / authors after their recent decease.\n",
    "The second assumption, is that this phenomenon can be reduced to the artistic and literary community, which are those concerned with mass celebrity. The last assumption is that the current means of communication allow the whole society concerned, in this case American, to know the news few time after the event. Especially if it is about well-known people.\n",
    "\n",
    "By working on data from Amazon, the giant of online commerce, and Wikipedia, the most famous encyclopedia of the web, it is possible to test the post-mortem worship effect.\n",
    "Indeed, the first part of this project consisted of the extraction of the data of interest from Amazon and Wikipedia. This required to filter Amazon data to contain only the required cathegories, clean it and store it in a convenient format for future implementations.\n",
    "Otherwise, the list of authors deceased in the time interval corresponding to Amazon's data, was scrapped from Wikipedia and stored in a compact and easy-to-use format.  \n",
    "The second part of the research will be based on the extraction of quantifiable features (interest in the form of number of reviews, appraisal index of reviews, temporal dimensionnality...) in order to allow a mathematical analysis of the data.\n",
    "The last conclusions will be drawn based on mathematical results and hypothesis testing.\n",
    "\n",
    "\n",
    "## Research questions\n",
    "* When a author/artist died, What trend of popularity occurs on their related product on amazon? (For an author; it's book, for an actor; related movies,... etc)\n",
    "* What's this impact in function of the type of artwork the author/artist did? (musics/books/films/...)\n",
    "\n",
    "\n",
    "## Dataset\n",
    "We want to use the Amazon datasets provided in the course, both the review and the metadata dataset. (So at most 20 + 3.1 gb in Json). \n",
    "But we will use only specific categories related the creation by an author/artist. (musics/books/films/...)\n",
    "Since we're very interested in the amount of reviews as a metric of interest, we will restrict our data to the 5-core dataset, as to have at least a few reviews per product.\n",
    "The interest rate in function of time will be computed with the help of the review content and their dates. (text analysis)\n",
    "To correlate this interest rate, we will need artists'/authors' death and their corresponding work. For this we will use Wikipedia and scrap the useful data needed for our project.\n",
    "One hard part will be to match  the works of an artist/author to corresponding product on amazon. \n",
    "\n",
    "\n",
    "## A list of internal milestones up until project milestone 2\n",
    "* Define the useful feature inside all the dataset\n",
    "* Select the categories of product in Amazon containing works of authors/artists (Amazon has 24 categories of item)\n",
    "* Scrap the death of artists/authors of the N last years, match it with all it's work, then match it with all corresponding amazon product.\n",
    "* Clean the data\n",
    "* Think about how to present the project in term of data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "# 2. Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='amazon'></a>\n",
    "## 2.1 Amazon Dataset\n",
    "\n",
    "Amazon has a lot of categories:\n",
    "\n",
    "Books, Electronics, Movies and TV, CDs and Vinyl, Clothing (Shoes and Jewelry), Home and Kitchen, Kindle Store, Sports and Outdoors, Cell Phones and Accessorie, Health and Personal Care, Toys and Games, Video Games, Tools and Home Improvement, Beauty, Apps for Android, Office Products, Pet Supplies, Automotive, Grocery and Gourmet Food, Patio (Lawn and Garden), Baby, Digital Music, Musical Instruments, Amazon Instant Video\n",
    "\n",
    "\n",
    "### For our project we consider the useful categories as:\n",
    "* Books\n",
    "* Movies and TV\n",
    "* CDs and Vinyl\n",
    "* Kindle Store\n",
    "* Digital Music\n",
    "* Amazon Instant Video\n",
    "\n",
    "#### Potentially useful\n",
    "* Toy and Games\n",
    "* Video Games\n",
    "\n",
    "---\n",
    "We can acess the amazon detaset review [here](http://jmcauley.ucsd.edu/data/amazon/links.html).\n",
    "\n",
    "##### Authors\n",
    "* [Books](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz)\n",
    "* [Kindle Store](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_5.json.gz)\n",
    "\n",
    "##### Actors\n",
    "* [Movies and TV](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz)\n",
    "* [Amazon Instant Video](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz)\n",
    "\n",
    "##### Musician\n",
    "* [CDs and Vinyl](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_5.json.gz)\n",
    "* [Digital Music](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_5.json.gz)\n",
    "\n",
    "\n",
    "#### Metadata (list of all the product with its description)\n",
    "* [Books](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Books.json.gz)\n",
    "* [Kindle Store](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Kindle_Store.json.gz)\n",
    "* [Movies and TV](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Movies_and_TV.json.gz)\n",
    "* [Amazon Instant Video](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Amazon_Instant_Video.json.gz)\n",
    "* [CDs and Vinyl](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_CDs_and_Vinyl.json.gz)\n",
    "* [Digital Music](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Digital_Music.json.gz)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_json(data_path+'Kindle_Store_5.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Right now we don't use the amazon dataset review at all; for the milestone #2 we only gather the data from wikipedia and amazon product api, to have to tools to do our analysis.\n",
    "\n",
    "Note that the files are at most 2GB, we can analyse them separately. Therefore, we don't have to use SPARK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wikipedia'></a>\n",
    "## 2.2 scraping Wikipedia\n",
    "\n",
    "We want to scrap all the musicians, actors and authors that died from 1994 to 2016, from wikipedia. Since we want the most notorious ones, we will scrap from [year's summary page](https://en.wikipedia.org/w/index.php?title=2000#Deaths), and not from the specific [\"death in 'year'\" page](https://en.wikipedia.org/wiki/Deaths_in_2000) (exemple for 2000).\n",
    "\n",
    "We will iterate for each wanted year page, and scrap the celebrity's name, birth date, description, and death date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that matchs a line of the wikitext and return an array of tuples: \n",
    "# (person_death_date, person_name, person_description, person_birth_date)\n",
    "# debug = True return the intermediary matching\n",
    "def matchLine(line, debug=False):\n",
    "    if debug:\n",
    "        print(line)\n",
    "    #match when a line contains only 1 celebrity\n",
    "    match_1_line = re.match( r'.*\\[\\[(.*?)\\]\\] (&ndash;|-|–).*?\\[\\[(.*?)\\]\\](.*)\\(b\\..*\\[\\[(.*?)\\]\\]\\)', line)\n",
    "    if match_1_line:\n",
    "        if debug:\n",
    "            print(match_1_line)\n",
    "        return [(match_1_line.group(1),match_1_line.group(3),match_1_line.group(4),match_1_line.group(5))]\n",
    "    #didn't find a match for 1 celebrity\n",
    "    else:\n",
    "        result = []\n",
    "        #consider it's a line with multiple celebrities dead on the same day, separated by \\n**\n",
    "        s = line.split(\"\\n**\")\n",
    "        #if the split didn't work, we return a matching error:\n",
    "        if len(s)==1:\n",
    "            print(\"No match found for: \"+ str(year) +\" \"+ line)\n",
    "        #the first split contains only the death date\n",
    "        match_date = re.match( r'\\[\\[(.*)\\]\\]',s[0]).group(1)\n",
    "        #iterate for each celebrities\n",
    "        for i in range(1,len(s)):\n",
    "            match_3_param = re.match( r'.*\\[\\[(.*?)\\]\\](.*)\\(b\\. ?\\[\\[(.*?)\\]\\]\\)', s[i])\n",
    "            if debug:\n",
    "                print(match_3_param)\n",
    "            #if the match is succesful, add the celebrity to the array\n",
    "            if match_3_param and match_date:\n",
    "                result.append((match_date,match_3_param.group(1),match_3_param.group(2),match_3_param.group(3)))\n",
    "            #otherwise return a matching error\n",
    "            else:\n",
    "                print(\"No match found for: \"+ str(year) +\" \"+ s[i])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month_list = ['January', 'February', 'March', 'April', 'May', \n",
    "              'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Function that return a standardized date in string %year-%month-%day\n",
    "def computeDate(year, md):\n",
    "    a = md.split(\" \")\n",
    "    if a[0] in month_list:\n",
    "        month = month_list.index(a[0])+1\n",
    "        day = a[1]\n",
    "        return str(year)+\"-\"+str(month).zfill(2)+\"-\"+str(day).zfill(2)\n",
    "    #return an date error that we couldn't convert\n",
    "    else:\n",
    "        print(\"Date error: \"+md+\" \"+str(year));\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found for: 1996  Victims of [[TWA Flight 800]]\n",
      "No match found for: 2001  2,996 people (2,977 victims and 19 hijackers) who died in the [[September 11 attacks]]\n",
      "No match found for: 2013 [[April 30]] (death announced on this date) &ndash; [[Deanna Durbin]], Canadian-born singer and actress (b. [[1921]])\n",
      "(3312, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Death Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Birth Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>Malladihalli Sri Raghavendra Swamiji</td>\n",
       "      <td>, indian yogi</td>\n",
       "      <td>1890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>Moshe Aryeh Freund</td>\n",
       "      <td>, israeli rabbi</td>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>Arleigh Burke</td>\n",
       "      <td>, american naval officer</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>Arthur Rudolph</td>\n",
       "      <td>, german rocket engineer</td>\n",
       "      <td>1906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>Karl Targownik</td>\n",
       "      <td>, hungarian psychiatrist and holocaust survivor</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996-01-05</td>\n",
       "      <td>Yahya Ayyash</td>\n",
       "      <td>, palestinian shaheed</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1996-01-05</td>\n",
       "      <td>Lincoln Kirstein</td>\n",
       "      <td>, american writer and impresario</td>\n",
       "      <td>1907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1996-01-05</td>\n",
       "      <td>Richard Versalle</td>\n",
       "      <td>, american operatic tenor</td>\n",
       "      <td>1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1996-01-07</td>\n",
       "      <td>Prime Minister of Hungary</td>\n",
       "      <td></td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1996-01-07</td>\n",
       "      <td>Tarō Okamoto</td>\n",
       "      <td>, japanese artist</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Death Date                                  Name  \\\n",
       "0  1996-01-01  Malladihalli Sri Raghavendra Swamiji   \n",
       "1  1996-01-01                    Moshe Aryeh Freund   \n",
       "2  1996-01-01                         Arleigh Burke   \n",
       "3  1996-01-01                        Arthur Rudolph   \n",
       "4  1996-01-02                        Karl Targownik   \n",
       "5  1996-01-05                          Yahya Ayyash   \n",
       "6  1996-01-05                      Lincoln Kirstein   \n",
       "7  1996-01-05                      Richard Versalle   \n",
       "8  1996-01-07             Prime Minister of Hungary   \n",
       "9  1996-01-07                          Tarō Okamoto   \n",
       "\n",
       "                                         Description Birth Date  \n",
       "0                                     , indian yogi        1890  \n",
       "1                                   , israeli rabbi        1894  \n",
       "2                          , american naval officer        1901  \n",
       "3                          , german rocket engineer        1906  \n",
       "4   , hungarian psychiatrist and holocaust survivor        1915  \n",
       "5                             , palestinian shaheed        1966  \n",
       "6                  , american writer and impresario        1907  \n",
       "7                         , american operatic tenor        1932  \n",
       "8                                                          1930  \n",
       "9                                 , japanese artist        1911  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an empty dataFrame\n",
    "columns = ['Death Date', 'Name', 'Description','Birth Date']\n",
    "df = pd.DataFrame(columns=columns) \n",
    "\n",
    "#the year of the interval of the amazon dataset\n",
    "starting_year = 1996\n",
    "ending_year = 2014\n",
    "\n",
    "#every year we scrap the corresponding wikipedia page\n",
    "for year in range(starting_year,ending_year+1):\n",
    "    #we retrieve only the wikitext using the parameter action=raw\n",
    "    #note that the api documentation advise to do this way if we are interested only by the wikitext\n",
    "    year_url = \"https://en.wikipedia.org/w/index.php?action=raw&title={}&maxlag=5\".format(year)\n",
    "    r = requests.get(year_url)\n",
    "    page_text = r.text\n",
    "    #we split the text to retrieve only the part about celebrities' deaths\n",
    "    if(\"== Deaths ==\" in page_text):\n",
    "        a = page_text.split(\"== Deaths ==\")[1]\n",
    "    elif(\"==Deaths==\" in page_text):\n",
    "        a = page_text.split(\"==Deaths==\")[1]\n",
    "    else:\n",
    "        a = None\n",
    "    #we set our starting text at the first month (i.e January)\n",
    "    if(\"=== January ===\" in page_text):\n",
    "        a = a.split(\"=== January ===\")[1]\n",
    "    elif(\"===January===\" in page_text):\n",
    "        a = a.split(\"===January===\")[1]\n",
    "    else:\n",
    "        a = None\n",
    "    #all the month are separated by \\n\\n, so we split and iterate over the 12\n",
    "    deaths = a.split(\"\\n\\n\")\n",
    "    for i in range(12):\n",
    "        #We have 2 case: 1 celebrity per date, or multiple celebrity per date, so we split in consequence,\n",
    "        #then we match the line into groups using pattern recognition.\n",
    "        s = deaths[i].split('* ',1)[1]\n",
    "        lines = s.split(\"\\n* \")\n",
    "        for line in lines:\n",
    "            res = matchLine(line)\n",
    "            for person in res:\n",
    "                    #when we finished matching, we format the date, then add the celebrity into the dataFrame\n",
    "                    date = computeDate(year,person[0])\n",
    "                    df.loc[len(df)]=[date,person[1],\" \"+person[2].lower(),person[3]]\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matcher has only 3 errors over 3313 matching, which is a good result.\n",
    "Most of those error is just a special change of format in the list, where they listed celebrities inside specific tragedies.\n",
    "In this case we will add them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we add only wanted celebrity (singer, actor,...)\n",
    "#TWA Flight 800\n",
    "df.loc[len(df)]=[\"1996-07-17\",\"Marcel Dadi\",\", French guitarist\".lower(),\"1951\"]\n",
    "df.loc[len(df)]=[\"1996-07-17\",\"David Hogan\",\", American composer\".lower(),\"1949\"]\n",
    "#Deanna Durbin\n",
    "df.loc[len(df)]=[\"2013-04-30\",\"Deanna Durbin\",\", Canadian-born singer and actress\".lower(),\"1921\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider to multi-index on name and birth date.\n",
    "Then we order them in fuction of their death date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is unique: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Death Date</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Malladihalli Sri Raghavendra Swamiji</th>\n",
       "      <th>1890</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>, indian yogi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moshe Aryeh Freund</th>\n",
       "      <th>1894</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>, israeli rabbi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arleigh Burke</th>\n",
       "      <th>1901</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>, american naval officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arthur Rudolph</th>\n",
       "      <th>1906</th>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>, german rocket engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karl Targownik</th>\n",
       "      <th>1915</th>\n",
       "      <td>1996-01-02</td>\n",
       "      <td>, hungarian psychiatrist and holocaust survivor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Death Date  \\\n",
       "Name                                 Birth Date               \n",
       "Malladihalli Sri Raghavendra Swamiji 1890        1996-01-01   \n",
       "Moshe Aryeh Freund                   1894        1996-01-01   \n",
       "Arleigh Burke                        1901        1996-01-01   \n",
       "Arthur Rudolph                       1906        1996-01-01   \n",
       "Karl Targownik                       1915        1996-01-02   \n",
       "\n",
       "                                                                                       Description  \n",
       "Name                                 Birth Date                                                     \n",
       "Malladihalli Sri Raghavendra Swamiji 1890                                           , indian yogi   \n",
       "Moshe Aryeh Freund                   1894                                         , israeli rabbi   \n",
       "Arleigh Burke                        1901                                , american naval officer   \n",
       "Arthur Rudolph                       1906                                , german rocket engineer   \n",
       "Karl Targownik                       1915         , hungarian psychiatrist and holocaust survivor   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.set_index(['Name', 'Birth Date'])\n",
    "print(\"Index is unique: \"+str(df.index.is_unique))\n",
    "df2.sort_values('Death Date',inplace = True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we removed the celebrities which died outside of the interval of the Amazon dataset (May 1996 - July 2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Death Date</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>David Easton</th>\n",
       "      <th>1917</th>\n",
       "      <td>2014-07-19</td>\n",
       "      <td>, canadian-american political scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>James Garner</th>\n",
       "      <th>1928</th>\n",
       "      <td>2014-07-19</td>\n",
       "      <td>, american actor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carlo Bergonzi</th>\n",
       "      <th>1924</th>\n",
       "      <td>2014-07-25</td>\n",
       "      <td>, italian tenor and actor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Francesco Marchisano</th>\n",
       "      <th>1929</th>\n",
       "      <td>2014-07-27</td>\n",
       "      <td>, italian cardinal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Julio Grondona</th>\n",
       "      <th>1931</th>\n",
       "      <td>2014-07-30</td>\n",
       "      <td>, argentinian football authority</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Death Date  \\\n",
       "Name                 Birth Date               \n",
       "David Easton         1917        2014-07-19   \n",
       "James Garner         1928        2014-07-19   \n",
       "Carlo Bergonzi       1924        2014-07-25   \n",
       "Francesco Marchisano 1929        2014-07-27   \n",
       "Julio Grondona       1931        2014-07-30   \n",
       "\n",
       "                                                               Description  \n",
       "Name                 Birth Date                                             \n",
       "David Easton         1917         , canadian-american political scientist   \n",
       "James Garner         1928                                , american actor   \n",
       "Carlo Bergonzi       1924                       , italian tenor and actor   \n",
       "Francesco Marchisano 1929                              , italian cardinal   \n",
       "Julio Grondona       1931                , argentinian football authority   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[(df2['Death Date'] >= '1996-05-01') & (df2['Death Date'] <= '2014-07-31')]\n",
    "df2.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to keep only **musicians, actors or authors.**\n",
    "Since we have a small description of the celebrity, it's trivial that the description will contains his job if the celebrity is famous by his job. \n",
    "_Note that some celebrities doesn't have a description, it's the case when their name contains the description: president, prince, king,...etc._\n",
    "\n",
    "We match a celebrity if he is a musician, actor or author if it contains a specific keyword, for example: _\"actor\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of celebrities: 3315\n",
      "Number of useful celebrities: 1315\n",
      "Number of unwanted celebrities: 2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Death Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Actor</th>\n",
       "      <th>Author</th>\n",
       "      <th>Musician</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th>Birth Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jack Weston</th>\n",
       "      <th>1924</th>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>, american actor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John Beradino</th>\n",
       "      <th>1917</th>\n",
       "      <td>1996-05-19</td>\n",
       "      <td>, american baseball player and actor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jon Pertwee</th>\n",
       "      <th>1919</th>\n",
       "      <td>1996-05-20</td>\n",
       "      <td>, british actor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paul Delph</th>\n",
       "      <th>1957</th>\n",
       "      <td>1996-05-21</td>\n",
       "      <td>, american musician and producer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lash LaRue</th>\n",
       "      <th>1917</th>\n",
       "      <td>1996-05-21</td>\n",
       "      <td>, american actor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Death Date                             Description  \\\n",
       "Name          Birth Date                                                       \n",
       "Jack Weston   1924        1996-05-03                       , american actor    \n",
       "John Beradino 1917        1996-05-19   , american baseball player and actor    \n",
       "Jon Pertwee   1919        1996-05-20                        , british actor    \n",
       "Paul Delph    1957        1996-05-21       , american musician and producer    \n",
       "Lash LaRue    1917        1996-05-21                       , american actor    \n",
       "\n",
       "                          Actor  Author  Musician  \n",
       "Name          Birth Date                           \n",
       "Jack Weston   1924         True   False     False  \n",
       "John Beradino 1917         True   False     False  \n",
       "Jon Pertwee   1919         True   False     False  \n",
       "Paul Delph    1957        False   False      True  \n",
       "Lash LaRue    1917         True   False     False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2\n",
    "\n",
    "#return true if the description contains one of this keywork\n",
    "jobMusician = [\"dj\",\"baritone\",\"bard\",\"pianist\",\"singer\",\"tenor \",\"soprano\", \"composer\",\"trumpeter\",\"saxophonist\",\"lyricist\", \"drummer\", \"musician\", \"rapper\",\"guitarist\",\"violinist\",\"violist\",\"bassist\"]\n",
    "def isMusician(s):\n",
    "    for job in jobMusician:\n",
    "        if job in s:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#return true if the description contains one of this keywork\n",
    "jobActor = [\"actor\", \"actress\",\"filmmaker\",\"cinematographer\",\"film director\", \"film producer\"]\n",
    "def isActor(s):\n",
    "    for job in jobActor:\n",
    "        if job in s:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#return true if the description contains one of this keywork\n",
    "jobAuthor = [\"autor\", \"author\", \"writer\", \"poet\", \"novelist\",\"cartoonist\",\"comic strip artist\",\"manga artist\"]\n",
    "def isAuthor(s):\n",
    "    for job in jobAuthor:\n",
    "        if job in s:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#add 3 column to the data frame: Actor, author and Musician. It's possible that someone is both, so they all have those 3 booleans parameters.\n",
    "df3 = df3.merge(df3.Description.apply(lambda s: pd.Series({'Musician':isMusician(s), 'Actor':isActor(s), 'Author':isAuthor(s)})), \n",
    "    left_index=True, right_index=True) \n",
    "\n",
    "#filter only actor author or musician\n",
    "df_artists = df3[(df3['Actor'] == True) | (df3['Author'] == True) | (df3['Musician'] == True)]\n",
    "df_artists = df_artists.sort_values('Death Date')\n",
    "\n",
    "print(\"Number of celebrities: \"+str(df.shape[0]))\n",
    "print(\"Number of useful celebrities: \"+str(df_artists.shape[0]))\n",
    "print(\"Number of unwanted celebrities: \"+str(df.shape[0]-df_artists.shape[0]))\n",
    "\n",
    "#print only the wanted celebrities\n",
    "df_artists.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save this DF, since wikipedia pages are edited frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_artists.to_csv('DATA/deaths.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='amazonapi'></a>\n",
    "## 2.3 Amazon Product API\n",
    "\n",
    "Now we want to link an amazon product to celebrities it is related to : book's authors, film's actors and directors, music's artist. \n",
    "\n",
    "Our first idea was to scrap his works from their personnal Wikipedia page, and scrap all their works. For example, for an actor, we would scrap the subsection \"Filmography\" and read the corresponding tables in its subsections to find the link between the actor and his films.\n",
    "\n",
    "However there exists an [Amazon product API](http://docs.aws.amazon.com/AWSECommerceService/latest/DG/becomingDev.html) that does the hard work for us. When you give a product ID (ASIN), the api will return a wrapper object, that contains field values for **directors, actors, authors, creators**. It's perfect for us! This way we will filter the products related to our artists list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As to the implementation, it is identical for each categories, but it takes a long time to run, as amazon requires that API users throttle their requests in order not to flood their servers.  \n",
    "We query all the necesary ASINs, then save the resulting dataframe to a .csv file. For this reason, we will show the code we used to gather the artists for the 'Movies and TV' category - it can be generalized to all other categories - and simply load the results we obtained for the other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'meta_Movies_and_TV.json.gz'\n",
    "features = ['asin', 'title', 'description']\n",
    "df_name = \"meta_Movies_and_TV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all ASINs we're going to query - use the metadata files\n",
    "## for this, as they contains each ASIN once and only once.\n",
    "##\n",
    "\n",
    "''' This function was provided on the amazon dataset's webpage\n",
    "    It loads a gzipped file directly into a dataframe\n",
    "'''\n",
    "def gz_to_dataframe(datapath, filename):\n",
    "    def parse(path): \n",
    "        g = gzip.open(path, 'rb') \n",
    "        for l in g: \n",
    "            yield eval(l) \n",
    "    def getDF(path): \n",
    "        i = 0 \n",
    "        df = {} \n",
    "        for d in parse(path): \n",
    "            df[i] = d \n",
    "            i += 1 \n",
    "        return pd.DataFrame.from_dict(df, orient='index') \n",
    "    return getDF(datapath+filename)\n",
    "    \n",
    "amazon_products_df = gz_to_dataframe(datapath, filename)[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sign in with amazon API \n",
    "##\n",
    "\n",
    "def get_amazon_interface():\n",
    "    f = open(\"api_creds\")\n",
    "    ar = f.read().split(\"\\n\")\n",
    "    return AmazonAPI(ar[0], ar[1], ar[2])\n",
    "    return ar[0], ar[1], ar[2]\n",
    "\n",
    "amazon = get_amazon_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Here, we define some API query helpers\n",
    "##\n",
    "\n",
    "''' Product lookup with API, asin can be a string ('one by one' lookup)\n",
    "    or a list of strings ('bulk lookup').\n",
    "    bulk lookup provides better performance\n",
    "'''\n",
    "def get_prod(asin) : \n",
    "    if not isinstance(asin, str): \n",
    "        acc_str = asin[0]\n",
    "        for e in asin : \n",
    "            acc_str += ','+e\n",
    "        print(acc_str)\n",
    "        return amazon.lookup(ItemId=acc_str)\n",
    "    else :\n",
    "        return amazon.lookup(ItemId=asin)\n",
    "    \n",
    "''' Splits the interval [start-end] into bulks of size bulksize\n",
    "'''    \n",
    "def gen_bulk_index(start, end, bulksize=10, includeEnd=False):\n",
    "    size = end - start + 1\n",
    "    bulks = [list(range(start+(i*bulksize), start + (i+1)*bulksize)) for i in range(0, int(size/bulksize))]\n",
    "    if includeEnd : \n",
    "        bulks.append(list(range(bulks[len(bulks)-1][bulksize-1], end+1)))\n",
    "    else : \n",
    "        bulks.append(list(range(bulks[len(bulks)-1][bulksize-1], end)))\n",
    "    return bulks    \n",
    "\n",
    "''' Helper : extracts wanted data from the amazon product 'amazonProduct' and \n",
    "             puts it in the dataframe at the given positions   \n",
    "'''\n",
    "def set_df_cell_from_product(amazonProduct, rowNumber, fieldName1, fieldName2) :\n",
    "    if (amazonProduct is None) : \n",
    "        product_df.set_value(rowNumber, fieldName1, [])\n",
    "        product_df.set_value(rowNumber, fieldName2, [])     \n",
    "    else : \n",
    "        actors = get_actors(amazonProduct)\n",
    "        product_df.set_value(rowNumber, fieldName1, actors)\n",
    "        directors = get_directors(amazonProduct)\n",
    "        product_df.set_value(rowNumber, fieldName2, directors)\n",
    "\n",
    "\n",
    "def get_directors(prod) : \n",
    "    return prod.directors\n",
    "\n",
    "def get_actors(prod) : \n",
    "    return prod.actors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Also, we have some functions to save the state of our data structure - in case we need to \n",
    "## to shutdown the computer and restart the query loop at a later time (not used here)\n",
    "##\n",
    "\n",
    "def save_progress(dataframe, nb_rows_processed):\n",
    "    dataframe.to_csv(datapath+df_name+\"temp.csv\")\n",
    "    file = open(datapath+df_name+\"progress\", \"w\")\n",
    "    file.write(str(nb_rows_processed))\n",
    "\n",
    "    \n",
    "def load_progress():\n",
    "    dataframe = pd.read_csv(datapath+df_name+\"_temp.csv\")\n",
    "    file = open(datapath+df_name+\"progress\", \"r\")\n",
    "    nb_rows_processed = file.readline()\n",
    "    return dataframe, int(nb_rows_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance by limiting network IO, the API allows \"bulk querying. We will use this feature, however, we found out the some query in the bulk could \"fail silently\" : you can query for 10 ASINs, and receive a list of only 8 responses, while if you query each ASIN independantly, a AsinNotFound exception would be raised for the two missing products.   \n",
    "To make up for that, we use bulk querying, but fall back to 'one-by-one' querying in case of errors. We also exploited the notebook \"variables are kept intact inbetween cell runs\" to save our results in case of unexpected network error, instead of putting all our code in a try/catch block, since network errors happened very rarely with proper throttling parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last item looked up :  0   -  time :  15:04:49 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Parameters & initialization for bulk item lookup\n",
    "##\n",
    "\n",
    "bulksize = 10\n",
    "\n",
    "# Change this to restore progress from file\n",
    "fresh_run = True\n",
    "\n",
    "if fresh_run : \n",
    "    # used to restart from where we were in case of unexepected network error\n",
    "    lastItemLookedUp = 0\n",
    "    amazon_products_df['actors'] = pd.Series(dtype=object)\n",
    "    amazon_products_df['directors'] = pd.Series(dtype=object)\n",
    "else : \n",
    "    amazon_products_df, lastItemLookedUp = load_progress()\n",
    "    \n",
    "print(\"last item looked up : \", lastItemLookedUp,  \"  -  time : \",time.strftime(\"%H:%M:%S\"), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 % completed ( 0  rows)   -  time :  15:05:34\n",
      "     Last Item Looked up :  0  /  208321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0000143561,0000143561,0000589012,0000695009,000107461X,0000143529,0000143502,0000143588,0001517791,0001527665,0001516035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-094b1a32ae54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# get asins for the bulk and fetch the matching AmazonProducts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0masins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamazon_products_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'asin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbulk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Then, process each product to add necessary informations in the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d2a7fa24b682>\u001b[0m in \u001b[0;36mget_prod\u001b[0;34m(asin)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0macc_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mamazon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mItemId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mamazon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mItemId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/amazon/api.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, ResponseGroup, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \"\"\"\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItemLookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseGroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mResponseGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjectify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsValid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'False'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/bottlenose/api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# make the actual API call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         response = self._call_api(api_url,\n\u001b[0;32m--> 265\u001b[0;31m                                   {'api_url': api_url, 'cache_url': cache_url})\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# decompress the response if need be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/bottlenose/api.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, api_url, err_env)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;31m# the simple way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mErrorHandler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Querying loop\n",
    "##\n",
    "\n",
    "original_lastItemLookedUp = lastItemLookedUp\n",
    "\n",
    "\n",
    "for bulk in gen_bulk_index(lastItemLookedUp, amazon_products_df.shape[0], bulksize=bulksize) : \n",
    "    # display progess\n",
    "    if ((bulk[0]-(original_lastItemLookedUp)) % 100 == 0) : \n",
    "        clear_output()\n",
    "        print(\"    \",int(100 * (bulk[0]+1) / amazon_products_df.shape[0]), \"% completed (\",bulk[0], \" rows)\", \"  -  time : \",time.strftime(\"%H:%M:%S\"))\n",
    "        print(\"     Last Item Looked up : \", lastItemLookedUp, \" / \", amazon_products_df.shape[0])\n",
    "        print(\"\\n\\n\\n\")\n",
    "    \n",
    "    # get asins for the bulk and fetch the matching AmazonProducts\n",
    "    asins = amazon_products_df['asin'][bulk].tolist()\n",
    "    prods = get_prod(asins)\n",
    "\n",
    "    # Then, process each product to add necessary informations in the dataframe\n",
    "    if (type(prods) is list) and (len(prods) == bulksize) :              \n",
    "        # Case : we found exactly one result per ASIN\n",
    "        #        process by bulk\n",
    "            for i, prod in enumerate(prods) : \n",
    "                set_df_values_from_product(prod, bulk[i], \"actors\", \"directors\")\n",
    "    elif (type(prods) is list) or (type(prods) is AmazonApi.AmazonProduct) :  \n",
    "        # Case : we obtained a list of AmazonProducts or a single AmazonProduct\n",
    "        #        fallback to 1-by-1 querying\n",
    "        for n in bulk :               \n",
    "            asin = product_df['asin'][n]\n",
    "            try : \n",
    "                prod = get_prod(asin)\n",
    "            except(AsinNotFound): \n",
    "                prod = None\n",
    "            set_df_values_from_product(prod, n, \"actors\", \"directors\")\n",
    "            time.sleep(0.15)\n",
    "        \n",
    "    # Save progress\n",
    "    lastItemLookedUp = bulk[bulksize-1]\n",
    "        \n",
    "    # limit query frequency to avoid 503 errors\n",
    "    time.sleep(min(bulksize/10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the resulting dataframe to file, and re-use it later by simply loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon_products_df.to_csv(datapath+df_name+\".csv\")\n",
    "del amazon_products_df  # free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='milestone3'></a>\n",
    "# 3. What's next for Milestone #3\n",
    "\n",
    "Currently we have all the review for each product, the list of celebrities and the list of product linked with celebrities name.\n",
    "We will mix everything together and analyse the patterns of review (date, rating,...etc) for each given artist and draw conclusions from it.\n",
    "\n",
    "We still want to answer those questions:\n",
    "* When a author/artist died, What trend of popularity occurs on their related product on amazon? (For an author; it's book, for an actor; related movies,... etc)\n",
    "* What's this impact in function of the type of artwork the author/artist did? (musics/books/films/...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
